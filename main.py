import random
import requests
from bs4 import BeautifulSoup
import urllib.request
import html5lib
import shutil
import string
import os
import json

#NOTE: I am not the best with commenting my code, far from it
#still practising it, apologizes if I screwed it up or didn't do it properly
print("""
|---------------------------------------------------------------------|
| Under no circumstances I will be held responsible in any way        |
|  for any claims, damanges, losses, costs or liabilities whatsoever. |
| This is shared only for educational purposes only and I do          |
|not encourage anyone to use it to harm or take information that it   |
| doesn't belong to them or they aren't authorised to use.            |
|---------------------------------------------------------------------|
""")
urls = [] #list to store urls in data.json
#checks if data.json exists: if not then creates it.
try:
    urls = json.loads(open('data.json').read())
    print(f"\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nFound Existing Data File: Containing {len(urls)} strings, using it.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
except FileNotFoundError:
    with open("data.json", "w") as dataFile:
        print("Data file not found, created.\n")
        dataFile.write("[]")
        urls = json.loads(open('data.json').read())

#checks if ./images/ path exists, if not then creates it.
isImages = os.path.isdir("./images/")
if isImages == True:
    print("Directory /images/ exists, good to go.\n")
else:
    os.mkdir("./images/")
    print("Created Directory /images/, the downloaded images will be stored there.\n")

#This chunk checks if all images listed in data.json are downloaded for naming purposes
#if all of them are downloaded then it will use length of data.json list for the naming
#if not then it will use length of files in directory as a starting point, for example
#if there are 6 images in /images/, the next one is going to be named 'img6.jpg'
#or if there are 0 images or files in directory it will start as 'img1.jpg'
filesInImagesDir = os.listdir("./images/")
namingStartPoint = 0
if len(filesInImagesDir) != len(urls):
     namingStartPoint = len(filesInImagesDir) + 1
     print("Not all urls in data file are downloaded, starting new naming session.\n")
else:
    print("All urls listed in data file are downloaded, using the default naming session.\n")
    namingStartPoint = len(urls)

CounterInput = int(input("How many images will you download?: ")) #user input on how many photos they will download

imgCounter = 0 #downloaded image counter for the loop
chars = string.ascii_letters #characters that will be randomply chosen for the url
errorCounter = 0 #counter for how many images encountered an error while dowloading

while imgCounter != CounterInput:
    base_url = 'https://prnt.sc/' #random letters will be put after this url, this url doesn't change
    random_letters = ''.join(random.choice(chars) for i in range(6)) #choses 6 random letters

    #checks if generated url is already used, if not then append it to the used list (urls)
    #then import the list in data.json file
    #if it's a duplicate it generates a new one and stores it in the list and imports it as well.
    if random_letters not in urls:
        urls.append(random_letters.lower())
        jsonList = json.dumps(urls)
        with open("data.json", "w") as dataFile:
            dataFile.write(jsonList)
        print("New URL.")
    else:
        print("Duplicate URL, regenerating...")
        random_letters = ''.join(random.choice(chars) for i in range(6))
        urls.append(random_letters.lower())
        jsonList = json.dumps(urls)
        with open("data.json", "w") as dataFile:
            dataFile.write(jsonList)

    url = base_url + random_letters.lower() #full orl to be used
    print(f"URL: {url}") #prints url to the user

    #opens webpage, using headers to avoid being blocked (Forbidden to access)
    #gets and stores raw html of the webpage
    page=urllib.request.Request(url,headers={'User-Agent': 'Mozilla/5.0'})
    infile=urllib.request.urlopen(page).read()
    data = infile.decode('ISO-8859-1')

    #gets images link from the raw html data,
    #breaks at first loop because we don't need the other images on the webpage
    #and the one we need is the first we get.
    soup = BeautifulSoup(data, 'html5lib')
    image_url = ""
    for link in soup.findAll('img'):
        image_url = link['src']
        break

    #names how will the image will be stored.
    filename = f"img{namingStartPoint}.png"
    namingStartPoint += 1

    #try except block if any error happens for some reason to avoid the script from breaking.
    try:
        #downloads the image
        r = requests.get(image_url, stream = True)
        if r.status_code == 200: #if status code is 200 it means that the image was downloaded, no errors happened.
            r.raw.decode_content = True
            path = f"./images/{filename}" #path to images folder, I'm using linux so I used '/', idk if this works on windows, if not replace '/' with '\'
            with open(path,'wb') as f: #saves the image using shutil library.
                shutil.copyfileobj(r.raw, f)

                print(f"Image Sucessfully Downloaded.\nImage Number: {imgCounter}\n----------------------")
                imgCounter += 1
        else:
            print(f"Image Number {imgCounter} Couldn't be retreived.")
            imgCounter += 1
            errorCounter +=1
    except:
        print("Some error happened, skipping.")
        errorCounter +=1

print(f"Script Finished with {CounterInput - errorCounter} images Downloaded, {errorCounter} Images couldn't be downloaded")
